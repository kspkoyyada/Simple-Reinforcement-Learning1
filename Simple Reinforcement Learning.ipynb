{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer the requirements document for installation of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "#A class Net is derived from nn.Module \n",
    "#Here we initialise our two fully connected layer that makes the core of our neural network. \n",
    "#The first fully connected layer (fc1) is our input layer which takes in a tensor the same size( 8 dimensional vector) as our state size and outputs a tensor that is the size of our hidden nodes (in this case its 200). \n",
    "#The second fully connected layer (fc2) is our hidden layer, it takes in the output from our previous layer and outputs a tensor that is the size of our action space (4 actions) it will output a number for each possible action our agent can take.\n",
    "#This class also contain a method forward which was called automatically whenver data has passed through network object\n",
    "#We pass that state through the first layer of our neural network and apply a ReLU activation function to the output of fc1. Next we take that output and pass it through our second layers. This value is then returned as the output of the whole network\n",
    "#Usually we would add an activation layer after the final output layer, such as the softmax function. The softmax function takes the numbers that were outputed for each possible action and normalises them so that they all add up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sessions\n",
    "#This is where we generate our batch of episodes. The agent will play through N episodes and gather the actions/states for each step so we can train our agent.\n",
    "#We define an activation function which takes the vector of probabilities as input from feedforward net defined in previous cell\n",
    "#defined  three lists to store our episode data: batchstates,batchactions(These are list of lists)\n",
    "say batchstates represents list of state sequences[[],[],.......[],[]]\n",
    "batchaction represents list of actionsequences[[],[],......[],[]]\n",
    "#batch_rewards stores the total reward achieved during each episode;[ .....] It is just a list gives rewards across batches.\n",
    "#Iterate through  batch size, running an episode for each iteration. \n",
    "#In our first loop we initialise two empty lists to store our actions/states for this episode. \n",
    "#We also created a variable to count the total reward of the episode. T\n",
    "#These are our data variables.\n",
    "#Finally we initialise our state variable 's' with a fresh episode by calling env.reset(), this will start a new game\n",
    "#Now we call a second loop that carrys out a single step in the game environment up until we reach our time limit for that episode. \n",
    "#First we need to get our current state and pass it through our network. \n",
    "#To do this we need to turn our state s into a torch float tensor so we can give it into the network. \n",
    "#Next we get the action probability from our network. \n",
    "#Remember we have to apply our activation function to the prediciton in order for the probabilities of the actions to all add up to 1 and be usable. \n",
    "#Once we have retrieved our probability distribution we can decide what action to take. This is done by using numpys random.choice function. \n",
    "#It will choose a “random” action based on the probabilities given. \n",
    "#Once we have decided upon which action to take, the action is carried out in the environment. \n",
    "#This will return the new state the reward recieved by taking that action, wether or not the episode is finished and any additional information the environment might provide. \n",
    "#Now that we have the information of our updated environment we need to add the state,action and reward to our data variables. #Finally we update our current state.\n",
    "#The last thing we need to do before is check if the episode has finished during this step. \n",
    "#If done is True we simply add our actions, states and rewards to their corresponding batch lists. Then break.\n",
    "#Once It is done just return our batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(env,batch_size, t_max=5000):\n",
    "    \n",
    "    activation = nn.Softmax(dim=1)\n",
    "    batch_actions,batch_states, batch_rewards = [],[],[]\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        states,actions = [],[]\n",
    "        total_reward = 0\n",
    "        s = env.reset()\n",
    "        for t in range(t_max):\n",
    "            \n",
    "            s_v = torch.FloatTensor([s])\n",
    "            act_probs_v = activation(net(s_v))\n",
    "            act_probs = act_probs_v.data.numpy()[0]\n",
    "            a = np.random.choice(len(act_probs), p=act_probs)\n",
    "\n",
    "            new_s, r, done, info = env.step(a)\n",
    "\n",
    "            #record sessions like you did before\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            total_reward += r\n",
    "\n",
    "            s = new_s\n",
    "            if done:\n",
    "                batch_actions.append(actions)\n",
    "                batch_states.append(states)\n",
    "                batch_rewards.append(total_reward)\n",
    "                break\n",
    "                \n",
    "    return batch_states, batch_actions, batch_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Elite Episodes\n",
    "#This method is used to select only the best episodes from the latest batch. \n",
    "#Find the reward threshold, in our case this is the top 20% or the 80th percentile but feel free to play around with that number, and then just take the episode data from episodes with a reward ≥ our reward threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(states_batch,actions_batch,rewards_batch,percentile=50):\n",
    "    \n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "    \n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(rewards_batch)):\n",
    "        if rewards_batch[i] > reward_threshold:\n",
    "            for j in range(len(states_batch[i])):\n",
    "                elite_states.append(states_batch[i][j])\n",
    "                elite_actions.append(actions_batch[i][j])\n",
    "    \n",
    "    return elite_states,elite_actions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "#batch_size: how many episodes to run at once\n",
    "#session_size: how many training epochs. each epoch runs one batch\n",
    "#percentile: used to determine our elite reward threshold\n",
    "#learning_rate: denotes how much we update our network by during each training step (need to find a good middle ground for this one)\n",
    "#completion_score: average reward over 100 episodes to be considered solved\n",
    "#Initialise the network we made previously\n",
    "#Choose a loss function\n",
    "#Choose an optimiser.\n",
    "#We run a loop for the number of sessions given. During each epoch(iteration) we run our generate_batch method to get our batch of episode data.\n",
    "#Now we filter out the bad episodes and keep the elite ones by calling our filter batch method.\n",
    "#Once we have the elite episodes that we want to train and we go through the process of passing data through our neural network.\n",
    "#Before each training step we need to set the gradients of our optimiser back to zero. \n",
    "#we’re reseting the optimiser. \n",
    "#Next we turn our elite_states and elite_actions lists into torch tensors so they can be used with our network.\n",
    "#We then pass all of the elite episode states into our network. \n",
    "#It goes through every state collected and predicts what the policy distribution should look like. \n",
    "#Next we compare these predictions to the actions that were carried out in our elite episodes. Ideally we want our networks predictions to be close to these.\n",
    "#To find out how far off our network was (the loss) we use the objective function (CrossEntropyLoss).\n",
    "#Once we have calculated the loss we use the backward method to calculate the gradients of our loss (backpropagation). \n",
    "#Finally optimizer updates our network by calling the step method.\n",
    "#The last thing to do is show the results and check if we achieved an average score that is higher than the completion score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.378, reward_mean=-168.7, reward_threshold=-105.9\n",
      "1: loss=1.359, reward_mean=-196.6, reward_threshold=-114.4\n",
      "2: loss=1.348, reward_mean=-146.4, reward_threshold=-91.7\n",
      "3: loss=1.333, reward_mean=-144.3, reward_threshold=-84.6\n",
      "4: loss=1.302, reward_mean=-125.9, reward_threshold=-70.7\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "#session_size = 500\n",
    "session_size =5\n",
    "percentile = 80\n",
    "hidden_size = 200\n",
    "completion_score = 200\n",
    "learning_rate = 0.01\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "n_states = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "#neural network\n",
    "net = Net(n_states, hidden_size, n_actions)\n",
    "#loss function\n",
    "objective = nn.CrossEntropyLoss()\n",
    "#optimisation function\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(session_size):\n",
    "    #generate new sessions\n",
    "    batch_states,batch_actions,batch_rewards = generate_batch(env, batch_size, t_max=5000)\n",
    "\n",
    "    elite_states, elite_actions = filter_batch(batch_states,batch_actions,batch_rewards,percentile)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    tensor_states = torch.FloatTensor(elite_states)\n",
    "    tensor_actions = torch.LongTensor(elite_actions)\n",
    "    action_scores_v = net(tensor_states)\n",
    "    loss_v = objective(action_scores_v, tensor_actions)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #show results\n",
    "    mean_reward, threshold = np.mean(batch_rewards), np.percentile(batch_rewards, percentile)\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_threshold=%.1f\" % (\n",
    "            i, loss_v.item(), mean_reward, threshold))\n",
    "    \n",
    "    #check if \n",
    "    if np.mean(batch_rewards)> completion_score:\n",
    "        print(\"Environment has been successfullly completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(net, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"LunarLander-v2\"), directory=\"videos\", force=True)\n",
    "generate_batch(env, 1, t_max=5000)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
